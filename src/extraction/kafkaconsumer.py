from confluent_kafka import Consumer, KafkaError
from confluent_kafka.admin import AdminClient
from transformation.datatransformer import process_and_group_data
import json

class KafkaConsumer:
    def __init__(self, bootstrap_servers, group_id, redis_loader, mongo_loader, sql_loader, batch_size=100):
        """
        Inicializa el consumidor de Kafka y los loaders.
        """
        self.conf = {
            'bootstrap.servers': bootstrap_servers,
            'group.id': group_id,
            'auto.offset.reset': 'earliest',
        }
        try:
            self.consumer = Consumer(self.conf)
            self.admin_client = AdminClient({'bootstrap.servers': bootstrap_servers})
            logger.info(f"âœ… ConexiÃ³n exitosa a Kafka en {bootstrap_servers}")
            print(f"âœ… ConexiÃ³n exitosa a Kafka en {bootstrap_servers}")
        except Exception as e:
            logger.error(f"âŒ Error al conectar con Kafka: {e}")
            print(f"âŒ Error al conectar con Kafka: {e}")
            raise e

        self.redis_loader = redis_loader
        self.mongo_loader = mongo_loader
        self.sql_loader = sql_loader
        self.batch_size = batch_size
        self.message_count = 0

    def start_consuming(self):
<<<<<<< Updated upstream
        # Paso 1: Obtener la lista de tÃ³picos
        metadata = self.admin_client.list_topics(timeout=10)
        print("TÃ³picos disponibles:")
        
        for topic in metadata.topics:
            print(f"TÃ³pico: {topic}")

        # Obtener el primer tÃ³pico encontrado
        if not metadata.topics:
            print("No hay tÃ³picos disponibles.")
            return

        primer_topico = list(metadata.topics.keys())[0]

        # Suscribir el consumidor al primer tÃ³pico
        self.consumer.subscribe([primer_topico])
        print(f"Suscrito al tÃ³pico: {primer_topico}")

=======
>>>>>>> Stashed changes
        try:
            metadata = self.admin_client.list_topics(timeout=10)
            available_topics = list(metadata.topics.keys())
            logger.info(f"ğŸ“‹ TÃ³picos disponibles: {available_topics}")
            print(f"ğŸ“‹ TÃ³picos disponibles: {available_topics}")
            
            if not available_topics:
                logger.error("âŒ No hay tÃ³picos disponibles")
                print("âŒ No hay tÃ³picos disponibles")
                return

            primer_topico = "probando"
            self.consumer.subscribe([primer_topico])
            logger.info(f"âœ… Suscrito al tÃ³pico: {primer_topico}")
            print(f"âœ… Suscrito al tÃ³pico: {primer_topico}")

            while True:
                msg = self.consumer.poll(1.0)
                if msg is None:
                    continue
                    
                if msg.error():
                    if msg.error().code() == KafkaError._PARTITION_EOF:
<<<<<<< Updated upstream
                        # Fin de la particiÃ³n
                        print("Fin de la particiÃ³n")
=======
                        logger.warning("âš ï¸ Fin de la particiÃ³n")
                        print("âš ï¸ Fin de la particiÃ³n")
>>>>>>> Stashed changes
                    else:
                        logger.error(f"âŒ Error Kafka: {msg.error()}")
                        print(f"âŒ Error Kafka: {msg.error()}")
                else:
                    self.message_count += 1
                    raw_message = msg.value().decode('utf-8')
                    
                    if self.message_count % 100 == 0:  # Mostrar cada 100 mensajes
                        logger.info(f"ğŸ“¨ Mensajes procesados: {self.message_count}")
                        print(f"ğŸ“¨ Mensajes procesados: {self.message_count}")
                    
                    try:
                        transformed_data = process_and_group_data(raw_message)
                        if "error" not in transformed_data:
                            buffer_full = self.redis_loader.add_to_buffer(transformed_data)
                            
                            if buffer_full:
                                logger.info("ğŸ”„ Buffer lleno - Iniciando procesamiento del batch")
                                print("ğŸ”„ Buffer lleno - Iniciando procesamiento del batch")
                                self.process_batch()
                        else:
                            logger.warning(f"âš ï¸ Mensaje invÃ¡lido: {transformed_data['error']}")
                            print(f"âš ï¸ Mensaje invÃ¡lido: {transformed_data['error']}")
                            
                    except Exception as e:
                        logger.error(f"âŒ Error al procesar mensaje: {e}")
                        print(f"âŒ Error al procesar mensaje: {e}")
                        logger.error(f"Mensaje que causÃ³ el error: {raw_message}")

        except KeyboardInterrupt:
<<<<<<< Updated upstream
            pass
=======
            logger.info("ğŸ‘‹ Deteniendo el consumidor por interrupciÃ³n del usuario")
            print("ğŸ‘‹ Deteniendo el consumidor por interrupciÃ³n del usuario")
        except Exception as e:
            logger.error(f"âŒ Error general en el consumidor: {e}")
            print(f"âŒ Error general en el consumidor: {e}")
>>>>>>> Stashed changes
        finally:
            logger.info("ğŸ”„ Procesando mensajes finales...")
            print("ğŸ”„ Procesando mensajes finales...")
            self.process_batch()
            self.consumer.close()
            self.redis_loader.close()
            self.mongo_loader.close()

    def process_batch(self):
        """
        Procesa un lote de datos desde Redis y los guarda en MongoDB y PostgreSQL.
        """
        try:
            batch_data = self.redis_loader.get_buffer_batch()
            
            if batch_data:
                logger.info(f"ğŸ“¦ Procesando batch de {len(batch_data)} mensajes")
                print(f"ğŸ“¦ Procesando batch de {len(batch_data)} mensajes")
                
                # Guardar en MongoDB
                self.save_messages_mongo(batch_data)
                # Guardar en PostgreSQL
                self.save_messages_sql(batch_data)
                
                logger.info("âœ… Batch procesado exitosamente")
                print("âœ… Batch procesado exitosamente")
            else:
                logger.warning("âš ï¸ No hay datos para procesar en el batch")
                print("âš ï¸ No hay datos para procesar en el batch")
                
        except Exception as e:
<<<<<<< Updated upstream
            print(f"Error al guardar en MongoDB: {e}")
            # Guardar los mensajes fallidos en un archivo de log para procesarlos mÃ¡s tarde
            with open('failed_messages.log', 'a') as log_file:
                for message in message_buffer:
                    log_file.write(json.dumps(message) + '\n')
=======
            logger.error(f"âŒ Error al procesar el batch: {e}")
            print(f"âŒ Error al procesar el batch: {e}")

    def save_messages_mongo(self, message_buffer):
        try:
            if not message_buffer:
                logger.warning("âš ï¸ Buffer vacÃ­o - No hay mensajes para guardar en MongoDB")
                return
                
            logger.info(f"ğŸ’¾ Guardando {len(message_buffer)} mensajes en MongoDB...")
            print(f"ğŸ’¾ Guardando {len(message_buffer)} mensajes en MongoDB...")
            
            inserted_count = self.mongo_loader.load_to_mongodb(message_buffer)
            
            if inserted_count:
                logger.info(f"âœ… {inserted_count} mensajes guardados en MongoDB")
                print(f"âœ… {inserted_count} mensajes guardados en MongoDB")
                    
        except Exception as e:
            logger.error(f"âŒ Error al guardar en MongoDB: {e}")
            print(f"âŒ Error al guardar en MongoDB: {e}")
>>>>>>> Stashed changes
        
    def save_messages_sql(self, message_buffer):
        try:
            logger.info(f"ğŸ’¾ Guardando {len(message_buffer)} mensajes en PostgreSQL...")
            print(f"ğŸ’¾ Guardando {len(message_buffer)} mensajes en PostgreSQL...")
            self.sql_loader.load_to_sql(message_buffer)
            logger.info("âœ… Datos guardados exitosamente en PostgreSQL")
            print("âœ… Datos guardados exitosamente en PostgreSQL")
        except Exception as e:
<<<<<<< Updated upstream
            print(f"Error al guardar en MongoDB: {e}")
            # Guardar los mensajes fallidos en un archivo de log para procesarlos mÃ¡s tarde
            with open('failed_messages.log', 'a') as log_file:
                for message in message_buffer:
                    log_file.write(json.dumps(message) + '\n')
    
    def save_messages(self):
        """
        Guarda el lote actual de mensajes en MongoDB y Postgres y maneja los errores si falla.
        """
        try:
            self.save_messages_mongo(self.message_buffer)
            self.save_messages_sql(self.message_buffer)
        except Exception as e:
            print(f"Exception as {e}:")
        
        finally:
            # Limpiar el buffer despuÃ©s de guardar
            self.message_buffer = []

# Esta clase es llamada e inicializada desde main.py, no directamente desde aquÃ­.
=======
            logger.error(f"âŒ Error al guardar en PostgreSQL: {e}")
            print(f"âŒ Error al guardar en PostgreSQL: {e}")
            for message in message_buffer:
                # Crear una copia del mensaje y eliminar campos no serializables
                safe_message = message.copy()
                if '_id' in safe_message:
                    safe_message['_id'] = str(safe_message['_id'])  # Convertir ObjectId a string
                logger.error(f"Mensaje fallido: {json.dumps(safe_message)}")
>>>>>>> Stashed changes
